---
editor_options: 
  chunk_output_type: console
---

# CTMM on WATLAS data using a computing cluster

This section is about running CTMM [@calabrese2016] on individual level movement data to calculate mean speeds and total distance travelled.
CTMM is a time intensive process taking _n log(n)_ seconds for _n_ positions (pers. comm. Mike Noonan).
We circumvent the obvious time cost by sending each individual's track to the RUG computing cluster for processing.

Methodologically, we read in raw, unprocessed data, remove so-called attractor points, and then clean the data by applying a 5-fix median filter.
We identify tidal cycles using data from Rijkswaterstaat, and split movement tracks into subsets lasting from one high tide to the next.
Treating these subsets as individual replicates in a population, as recommended [@calabrese2016], we guess CTMM parameters and fit CTMM to each tidal cycle.
Finally, we output the fitted models and their summaries as `.Rdata` and text files, respectively.

Much of the preliminary processing also happens on the cluster, and uses the the [WATLAS Utilities package](https://github.com/pratikunterwegs/watlasUtils).

**Workflow**

1. Prepare required libraries.
2. Read in data, apply the cleaning function, and overwrite local data.

```{r load_libs, eval=FALSE}
# load libs
library(data.table)
library(glue)
library(stringr)
library(tibble)
library(dplyr)
library(tidyr)
library(forcats)

# plot libs
library(ggplot2)
library(ggthemes)
library(scico)

# ssh func
library(ssh)
```

## Load prelim data

```{r prep_data, eval=FALSE}
# load data
# make a list of data files to read
data_files <- list.files(path = "data/watlas", pattern = "whole_season*", full.names = TRUE)

data_ids <- str_extract(data_files, "(tx_\\d+)") %>% str_sub(-3,-1)

# read deployment data from local file in data folder
tag_info <- fread("data/SelinDB.csv")

# filter out NAs in release date and time
tag_info <- tag_info[!is.na(Release_Date) & !is.na(Release_Time),]

# make release date column as POSIXct
tag_info[,Release_Date := as.POSIXct(paste(Release_Date, Release_Time, sep = " "), 
                                     format = "%d.%m.%y %H:%M", tz = "CET")]

# sub for knots in data
data_files <- data_files[as.integer(data_ids) %in% tag_info$Toa_Tag]
data_ids <- str_extract(data_files, "(tx_\\d+)") %>% str_sub(-3,-1)

```

## Choose scales of aggregation

```{r prep_agg_scale, eval=FALSE}
scales <- 30

data_to_test <- crossing(scales, nesting(data_files, data_ids))
rm(scales, data_files)
```

## Transfer data and jobs to cluster

```{r send_data, eval=FALSE}
# read password
password = fread("data/password.txt")$password

# transfer code files
{
  s <- ssh_connect("p284074@peregrine.hpc.rug.nl", passwd = password)
  rfiles <- list.files("code", pattern = ".r", full.names = TRUE)
  scp_upload(s, rfiles, to = "code/")
  ssh_disconnect(s)
}

# clear old speed estimates
{
  s <- ssh_connect("p284074@peregrine.hpc.rug.nl", passwd = password)
  ssh_exec_wait(s, command = "rm output/speed_estimates_2018.csv")
  ssh_disconnect(s)
}

# execute tests
for(i in 1:nrow(data_to_test)){
  
  scale <- data_to_test$scales[i]
  file <- data_to_test$data_files[i]
  id <- data_to_test$data_ids[i]
  # connect to peregrine
  s <- ssh_connect("p284074@peregrine.hpc.rug.nl", passwd = password)
  # make directory if non-existent
  ssh_exec_wait(s, command = "mkdir -p data/watlas")
  
  # list files already present
  files_on_prg <- ssh_exec_internal(s, command = "ls data/watlas")
  files_on_prg <- rawToChar(files_on_prg$stdout) %>% 
    str_split("\n") %>% 
    unlist()
  
  # check name
  data_name <- file %>% 
    str_split("/") %>% 
    unlist() %>% .[3]
  
  if(!data_name %in% files_on_prg){
    # upload data file for processing
    scp_upload(s, file, to = "data/watlas")
  }
  
  # make job file
  {
    shebang <- readLines("code/template_job.sh")
    
    # rename job
    shebang[2] <- glue('#SBATCH --job-name=ctmm_{file}')
    
    text <- glue('Rscript --vanilla code/code_test_ctmm_scale.r {file} {scale}')
    jobfile <- glue('code/job_ctmm_{id}_{scale}.sh')
    
    writeLines(c(shebang, text), con = jobfile)
    scp_upload(s, jobfile, to = "code/")
  }
  
  ssh_exec_wait(s, command = glue('dos2unix {jobfile}'))
  # process using ctmm
  ssh_exec_wait(s, command = glue('sbatch {jobfile}'))
  
  # disconnect
  ssh_disconnect(s)
  
}
```

## Access data from cluster

```{r get_speed_estimates, eval=FALSE}
# access data from cluster
{
  s <- ssh_connect("p284074@peregrine.hpc.rug.nl", passwd = password)
  scp_download(s, files = "output/speed_estimates_2018.csv", to = "data/mod_output/")
  ssh_disconnect(s)
}

# read data and process for plotting
{
  data <- fread("data/mod_output/speed_estimates_2018.csv", fill = T)
  data <- setDF(data) %>% 
    as_tibble() %>% 
    filter(!is.na(id)) %>% 
    rename(mean = est) %>% 
    pivot_longer(cols = c(low, mean, high),
                       names_to = "estimate", values_to = "speed") %>% 
    filter(!is.infinite(speed)) %>% 
    mutate(estimate = as.factor(estimate),
           estimate = fct_relevel(estimate, "low", "mean", "high"))
}
```

## Plot speeds over tides

```{r plot_speed_tide, eval=FALSE}
# plot data as lines
fig_speed_tide <- 
  ggplot(data)+
  geom_jitter(aes(x = tide_number, y = speed, group = id,
                col = factor(estimate)), size = 0.1, alpha = 0.5)+
  
  # geom_boxplot(aes(x = scale, y = speed, group = interaction(scale),
  #               col = factor(estimate)), size = 0.3, notch = F, 
  #              fill = "grey90", alpha= 0.5)+
  facet_wrap(~estimate, labeller = label_both, ncol = 1)+
  # scale_x_continuous(breaks = c(15,30,60))+
  scale_colour_scico_d(palette = "berlin", begin = 0.2, end = 0.8)+
  coord_cartesian(ylim = c(0,5))+
  theme_few()+
  theme(legend.position = "none")+
  labs(x = "tidal cycle (#)", y = "mean speed (m/s) foraging period")

# save figure
ggsave(fig_speed_tide, filename = "figs/fig_speed_tide.png", height = 5, width = 8, dpi=300)
dev.off()
```
