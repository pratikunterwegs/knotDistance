[
["index.html", "Linking experimental measures of personality to free-living movement Section 1 Introduction 1.1 Attribution 1.2 Data access 1.3 Data processing", " Linking experimental measures of personality to free-living movement Pratik R Gupte 2020-02-21 Section 1 Introduction This is the bookdown version of a project in preparation that links high-resolution tracking data from individual red knots Calidris canutus islandica to fine-scale experimental behaviour measurements in captivity, and aims to explore whether free-living movement can be predicted by consistent individual differences. 1.1 Attribution Please contact the following before cloning or in case of interest in the project. Selin Ersoy (lead author) Pratik Gupte (author and maintainer) PhD student, GELIFES – University of Groningen Guest researcher, COS – NIOZ p.r.gupte@rug.nl Nijenborgh 7/5172.0583 9747AG Groningen Allert Bijleveld (PI): allert.bijleveld@nioz.nl Project information: https://www.nioz.nl/en/about/cos/coastal-movement-ecology/shorebird-tracking 1.2 Data access The data used in this work are not publicly available. Contact PI Allert Bijleveld for data access. 1.3 Data processing The data processing for this project is described in the following sections. Navigate through them using the links in the sidebar. "],
["ctmm-on-watlas-data-using-a-computing-cluster.html", "Section 2 CTMM on WATLAS data using a computing cluster 2.1 Load prelim data 2.2 Choose scales of aggregation 2.3 Transfer data and jobs to cluster 2.4 Access data from cluster 2.5 Plot speeds over tides", " Section 2 CTMM on WATLAS data using a computing cluster This section is about running CTMM (???) on individual level movement data to calculate mean speeds and total distance travelled. CTMM is a time intensive process taking n log(n) seconds for n positions (pers. comm. Mike Noonan). We circumvent the obvious time cost by sending each individual’s track to the RUG computing cluster for processing. Methodologically, we read in raw, unprocessed data, remove so-called attractor points, and then clean the data by applying a 5-fix median filter. We identify tidal cycles using data from Rijkswaterstaat, and split movement tracks into subsets lasting from one high tide to the next. Treating these subsets as individual replicates in a population, as recommended (???), we guess CTMM parameters and fit CTMM to each tidal cycle. Finally, we output the fitted models and their summaries as .Rdata and text files, respectively. Much of the preliminary processing also happens on the cluster, and uses the the WATLAS Utilities package. Workflow Prepare required libraries. Read in data, apply the cleaning function, and overwrite local data. # load libs library(data.table) library(glue) library(stringr) library(tibble) library(dplyr) library(tidyr) library(forcats) # plot libs library(ggplot2) library(ggthemes) library(scico) # ssh func library(ssh) 2.1 Load prelim data # load data # make a list of data files to read data_files &lt;- list.files(path = &quot;data/watlas&quot;, pattern = &quot;whole_season*&quot;, full.names = TRUE) data_ids &lt;- str_extract(data_files, &quot;(tx_\\\\d+)&quot;) %&gt;% str_sub(-3,-1) # read deployment data from local file in data folder tag_info &lt;- fread(&quot;data/SelinDB.csv&quot;) # filter out NAs in release date and time tag_info &lt;- tag_info[!is.na(Release_Date) &amp; !is.na(Release_Time),] # make release date column as POSIXct tag_info[,Release_Date := as.POSIXct(paste(Release_Date, Release_Time, sep = &quot; &quot;), format = &quot;%d.%m.%y %H:%M&quot;, tz = &quot;CET&quot;)] # sub for knots in data data_files &lt;- data_files[as.integer(data_ids) %in% tag_info$Toa_Tag] data_ids &lt;- str_extract(data_files, &quot;(tx_\\\\d+)&quot;) %&gt;% str_sub(-3,-1) 2.2 Choose scales of aggregation scales &lt;- 30 data_to_test &lt;- crossing(scales, nesting(data_files, data_ids)) rm(scales, data_files) 2.3 Transfer data and jobs to cluster # read password password = fread(&quot;data/password.txt&quot;)$password # transfer code files { s &lt;- ssh_connect(&quot;p284074@peregrine.hpc.rug.nl&quot;, passwd = password) rfiles &lt;- list.files(&quot;code&quot;, pattern = &quot;.r&quot;, full.names = TRUE) scp_upload(s, rfiles, to = &quot;code/&quot;) ssh_disconnect(s) } # clear old speed estimates { s &lt;- ssh_connect(&quot;p284074@peregrine.hpc.rug.nl&quot;, passwd = password) ssh_exec_wait(s, command = &quot;rm output/speed_estimates_2018.csv&quot;) ssh_disconnect(s) } # execute tests for(i in 1:nrow(data_to_test)){ scale &lt;- data_to_test$scales[i] file &lt;- data_to_test$data_files[i] id &lt;- data_to_test$data_ids[i] # connect to peregrine s &lt;- ssh_connect(&quot;p284074@peregrine.hpc.rug.nl&quot;, passwd = password) # make directory if non-existent ssh_exec_wait(s, command = &quot;mkdir -p data/watlas&quot;) # list files already present files_on_prg &lt;- ssh_exec_internal(s, command = &quot;ls data/watlas&quot;) files_on_prg &lt;- rawToChar(files_on_prg$stdout) %&gt;% str_split(&quot;\\n&quot;) %&gt;% unlist() # check name data_name &lt;- file %&gt;% str_split(&quot;/&quot;) %&gt;% unlist() %&gt;% .[3] if(!data_name %in% files_on_prg){ # upload data file for processing scp_upload(s, file, to = &quot;data/watlas&quot;) } # make job file { shebang &lt;- readLines(&quot;code/template_job.sh&quot;) # rename job shebang[2] &lt;- glue(&#39;#SBATCH --job-name=ctmm_{file}&#39;) text &lt;- glue(&#39;Rscript --vanilla code/code_test_ctmm_scale.r {file} {scale}&#39;) jobfile &lt;- glue(&#39;code/job_ctmm_{id}_{scale}.sh&#39;) writeLines(c(shebang, text), con = jobfile) scp_upload(s, jobfile, to = &quot;code/&quot;) } ssh_exec_wait(s, command = glue(&#39;dos2unix {jobfile}&#39;)) # process using ctmm ssh_exec_wait(s, command = glue(&#39;sbatch {jobfile}&#39;)) # disconnect ssh_disconnect(s) } 2.4 Access data from cluster # access data from cluster { s &lt;- ssh_connect(&quot;p284074@peregrine.hpc.rug.nl&quot;, passwd = password) scp_download(s, files = &quot;output/speed_estimates_2018.csv&quot;, to = &quot;data/mod_output/&quot;) ssh_disconnect(s) } # read data and process for plotting { data &lt;- fread(&quot;data/mod_output/speed_estimates_2018.csv&quot;, fill = T) data &lt;- setDF(data) %&gt;% as_tibble() %&gt;% filter(!is.na(id)) %&gt;% rename(mean = est) %&gt;% pivot_longer(cols = c(low, mean, high), names_to = &quot;estimate&quot;, values_to = &quot;speed&quot;) %&gt;% filter(!is.infinite(speed)) %&gt;% mutate(estimate = as.factor(estimate), estimate = fct_relevel(estimate, &quot;low&quot;, &quot;mean&quot;, &quot;high&quot;)) } 2.5 Plot speeds over tides # plot data as lines fig_speed_tide &lt;- ggplot(data)+ geom_jitter(aes(x = tide_number, y = speed, group = id, col = factor(estimate)), size = 0.1, alpha = 0.5)+ # geom_boxplot(aes(x = scale, y = speed, group = interaction(scale), # col = factor(estimate)), size = 0.3, notch = F, # fill = &quot;grey90&quot;, alpha= 0.5)+ facet_wrap(~estimate, labeller = label_both, ncol = 1)+ # scale_x_continuous(breaks = c(15,30,60))+ scale_colour_scico_d(palette = &quot;berlin&quot;, begin = 0.2, end = 0.8)+ coord_cartesian(ylim = c(0,5))+ theme_few()+ theme(legend.position = &quot;none&quot;)+ labs(x = &quot;tidal cycle (#)&quot;, y = &quot;mean speed (m/s) foraging period&quot;) # save figure ggsave(fig_speed_tide, filename = &quot;figs/fig_speed_tide.png&quot;, height = 5, width = 8, dpi=300) dev.off() "],
["applying-ctmm-to-watlas-data.html", "Section 3 Applying CTMM to WATLAS data 3.1 Load libraries 3.2 Load prelim data 3.3 Choose scales of aggregation 3.4 Transfer data and process on cluster 3.5 Get speed estimates from cluster 3.6 Get instantaneous speed estimates", " Section 3 Applying CTMM to WATLAS data Here, we examine the effect of different aggregation scales on CTMM fits and speed estimates. 3.1 Load libraries # load libs library(data.table) library(glue) library(stringr) library(tibble) library(dplyr) library(tidyr) library(forcats) # plot libs library(ggplot2) library(ggthemes) library(scico) # ssh func library(ssh) 3.2 Load prelim data # load data # make a list of data files to read data_files &lt;- list.files(path = &quot;data/watlas&quot;, pattern = &quot;whole_season*&quot;, full.names = TRUE) data_ids &lt;- str_extract(data_files, &quot;(tx_\\\\d+)&quot;) %&gt;% str_sub(-3,-1) # read deployment data from local file in data folder tag_info &lt;- fread(&quot;data/SelinDB.csv&quot;) # filter out NAs in release date and time tag_info &lt;- tag_info[!is.na(Release_Date) &amp; !is.na(Release_Time),] # make release date column as POSIXct tag_info[,Release_Date := as.POSIXct(paste(Release_Date, Release_Time, sep = &quot; &quot;), format = &quot;%d.%m.%y %H:%M&quot;, tz = &quot;CET&quot;)] # sub for knots in data data_files &lt;- data_files[as.integer(data_ids) %in% tag_info$Toa_Tag][1:10] data_ids &lt;- str_extract(data_files, &quot;(tx_\\\\d+)&quot;) %&gt;% str_sub(-3,-1) 3.3 Choose scales of aggregation scales &lt;- c(30) data_to_test &lt;- crossing(scales, nesting(data_files, data_ids)) rm(scales, data_files) 3.4 Transfer data and process on cluster # read password password = fread(&quot;data/password.txt&quot;)$password # transfer code files { s &lt;- ssh_connect(&quot;p284074@peregrine.hpc.rug.nl&quot;, passwd = password) rfiles &lt;- list.files(&quot;code&quot;, pattern = &quot;.r&quot;, full.names = TRUE) scp_upload(s, rfiles, to = &quot;code/&quot;) ssh_disconnect(s) } # clear old speed estimates { s &lt;- ssh_connect(&quot;p284074@peregrine.hpc.rug.nl&quot;, passwd = password) ssh_exec_wait(s, command = &quot;rm output/speed_estimates_2018.csv&quot;) ssh_disconnect(s) } # execute tests for(i in 1:nrow(data_to_test)){ scale &lt;- data_to_test$scales[i] file &lt;- data_to_test$data_files[i] id &lt;- data_to_test$data_ids[i] # connect to peregrine s &lt;- ssh_connect(&quot;p284074@peregrine.hpc.rug.nl&quot;, passwd = password) # make directory if non-existent ssh_exec_wait(s, command = &quot;mkdir -p data/watlas&quot;) # list files already present files_on_prg &lt;- ssh_exec_internal(s, command = &quot;ls data/watlas&quot;) files_on_prg &lt;- rawToChar(files_on_prg$stdout) %&gt;% str_split(&quot;\\n&quot;) %&gt;% unlist() # check name data_name &lt;- file %&gt;% str_split(&quot;/&quot;) %&gt;% unlist() %&gt;% .[3] if(!data_name %in% files_on_prg){ # upload data file for processing scp_upload(s, file, to = &quot;data/watlas&quot;) } # make job file { shebang &lt;- readLines(&quot;code/template_job.sh&quot;) # rename job shebang[2] &lt;- glue(&#39;#SBATCH --job-name=ctmm_{file}&#39;) text &lt;- glue(&#39;Rscript --vanilla code/code_test_ctmm_scale.r {file} {scale}&#39;) jobfile &lt;- glue(&#39;code/job_ctmm_{id}_{scale}.sh&#39;) writeLines(c(shebang, text), con = jobfile) scp_upload(s, jobfile, to = &quot;code/&quot;) } ssh_exec_wait(s, command = glue(&#39;dos2unix {jobfile}&#39;)) # process using ctmm ssh_exec_wait(s, command = glue(&#39;sbatch {jobfile}&#39;)) # disconnect ssh_disconnect(s) } 3.5 Get speed estimates from cluster # access data from cluster { s &lt;- ssh_connect(&quot;p284074@peregrine.hpc.rug.nl&quot;, passwd = password) scp_download(s, files = &quot;output/speed_estimates_2018.csv&quot;, to = &quot;data/mod_output/&quot;) ssh_disconnect(s) } # read data and process for plotting { data &lt;- fread(&quot;data/mod_output/speed_estimates_2018.csv&quot;, fill = T) data &lt;- setDF(data) %&gt;% as_tibble() %&gt;% filter(!is.na(id)) %&gt;% rename(mean = est) %&gt;% pivot_longer(cols = c(low, mean, high), names_to = &quot;estimate&quot;, values_to = &quot;speed&quot;) %&gt;% filter(!is.infinite(speed)) %&gt;% mutate(estimate = as.factor(estimate), estimate = fct_relevel(estimate, &quot;low&quot;, &quot;mean&quot;, &quot;high&quot;)) } 3.5.1 Plot speed at different scales # plot data as lines fig_compare_ctmm &lt;- ggplot(data)+ geom_line(aes(x = tide_number, y = speed, group = interaction(id, tide_number), col = factor(estimate)), size = 0.1)+ geom_boxplot(aes(x = scale, y = speed, group = interaction(scale), col = factor(estimate)), size = 0.3, notch = F, fill = &quot;grey90&quot;, alpha= 0.5)+ facet_grid(id~estimate, labeller = label_both)+ scale_x_continuous(breaks = c(15,30,60))+ scale_colour_scico_d(palette = &quot;berlin&quot;, begin = 0.2, end = 0.8)+ coord_cartesian(ylim = c(0,10))+ theme_few()+ theme(legend.position = &quot;none&quot;)+ labs(x = &quot;aggregation interval (s)&quot;, y = &quot;mean speed (m/s) foraging period&quot;) # save figure ggsave(fig_compare_ctmm, filename = &quot;figs/fig_compare_scale.png&quot;, height = 5, width = 8, dpi=300) dev.off() knitr::include_graphics(&quot;figs/fig_compare_scale.png&quot;) (#fig:show_fig_compare_scale)Effect of aggregation scale on CTMM speed estimates, using 2 individual red knots over all tidal cycles in which they were tracked, and which satisfied quality criteria. 3.5.2 Plot speed in relation to proportion of positions # figure speed estimate ~ quality fig_speed_quality &lt;- ggplot(data %&gt;% filter(speed &lt;= 5))+ geom_jitter(aes(x = prop_fixes, y = speed, group = id, col = factor(estimate)), size = 0.1, alpha = 0.5)+ geom_smooth(aes(x = prop_fixes, y = speed, col = factor(estimate)), alpha = 0.5, method = &quot;lm&quot;)+ # geom_boxplot(aes(x = scale, y = speed, group = interaction(scale), # col = factor(estimate)), size = 0.3, notch = F, # fill = &quot;grey90&quot;, alpha= 0.5)+ facet_wrap(~estimate, labeller = label_both, ncol = 3)+ # scale_x_continuous(breaks = c(15,30,60))+ scale_colour_scico_d(palette = &quot;berlin&quot;, begin = 0.2, end = 0.8)+ coord_fixed(ylim = c(0,5), ratio = 0.1)+ theme_few()+ theme(legend.position = &quot;none&quot;)+ labs(x = &quot;prop. expected positions&quot;, y = &quot;mean speed (m/s) foraging period&quot;) # save figure ggsave(fig_speed_quality, filename = &quot;figs/fig_speed_quality.png&quot;, height = 5, width = 8, dpi=300) dev.off() knitr::include_graphics(&quot;figs/fig_speed_quality.png&quot;) (#fig:show_fig_compare_qual)Effect of proportion of positions realised on CTMM speed estimates. 3.6 Get instantaneous speed estimates # download model data folder { s &lt;- ssh_connect(&quot;p284074@peregrine.hpc.rug.nl&quot;, passwd = password) scp_download(s, files = &quot;output/mods&quot;, to = &quot;data/mod_output/&quot;) ssh_disconnect(s) } # list rdata files speed_files &lt;- list.files(&quot;data/mod_output/mods&quot;, pattern = &quot;speeds&quot;, full.names = TRUE) # read in griend library(sf) griend &lt;- st_read(&quot;data/griend_polygon/&quot;) buffer &lt;- griend %&gt;% st_buffer(dist = 2000) # read all data data &lt;- purrr::map_df(speed_files, function(df){ a &lt;- fread(df) setDF(a) a &lt;- a %&gt;% select(id, tide_number, x,y,est) %&gt;% filter(is.finite(est)) %&gt;% rename(tag = id) return(a) }) # plot inviduals per tide fig_speed_example &lt;- ggplot(data)+ geom_sf(data = griend)+ geom_path(aes(x, y, col = est, group = tide_number, size = est &gt; 5), alpha = 0.2)+ geom_point(aes(x, y, col = est, group = tide_number, size = est &gt; 5), alpha = 0.5)+ facet_wrap(~tag, labeller = label_both)+ theme_few()+ scale_size_manual(values = c(0.2, 0.1), guide = F)+ scale_colour_scico(palette = &quot;hawaii&quot;, limits = c(0, 5), na.value = &quot;red&quot;, direction = -1)+ theme(axis.text = element_blank())+ coord_sf(xlim = st_bbox(buffer)[c(1,3)], y = st_bbox(buffer)[c(2,4)])+ labs(x=NULL, y = NULL, col = &quot;speed m/s&quot;) # save figure ggsave(fig_speed_example, filename = &quot;figs/fig_speed_example.png&quot;, height = 12, width = 12, dpi=300) dev.off() knitr::include_graphics(&quot;figs/fig_speed_example.png&quot;) (#fig:show_fig_speed_map)Instantaenous speeds visualised "]
]
