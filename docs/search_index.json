[
["index.html", "Linking experimental measures of personality to free-living movement Section 1 Introduction 1.1 Attribution 1.2 Data access 1.3 Data processing", " Linking experimental measures of personality to free-living movement Pratik R Gupte 2020-02-15 Section 1 Introduction This is the bookdown version of a project in preparation that links high-resolution tracking data from individual red knots Calidris canutus islandica to fine-scale experimental behaviour measurements in captivity, and aims to explore whether free-living movement can be predicted by consistent individual differences. 1.1 Attribution Please contact the following before cloning or in case of interest in the project. Selin Ersoy (lead author) Pratik Gupte (author and maintainer) PhD student, GELIFES – University of Groningen Guest researcher, COS – NIOZ p.r.gupte@rug.nl Nijenborgh 7/5172.0583 9747AG Groningen Allert Bijleveld (PI): allert.bijleveld@nioz.nl Project information: https://www.nioz.nl/en/about/cos/coastal-movement-ecology/shorebird-tracking 1.2 Data access The data used in this work are not publicly available. Contact PI Allert Bijleveld for data access. 1.3 Data processing The data processing for this project is described in the following sections. Navigate through them using the links in the sidebar. "],
["ctmm-on-watlas-data-using-a-computing-cluster.html", "Section 2 CTMM on WATLAS data using a computing cluster 2.1 Prepare watlasUtils and other libraries 2.2 Prepare to remove attractor points, clean data and add tides 2.3 Read, clean, and write data", " Section 2 CTMM on WATLAS data using a computing cluster This section is about running CTMM (???) on individual level movement data to calculate mean speeds and total distance travelled. CTMM is a time intensive process taking n log(n) seconds for n positions (pers. comm. Mike Noonan). We circumvent the obvious time cost by sending each individual’s track to the RUG computing cluster for processing. Methodologically, we read in raw, unprocessed data, remove so-called attractor points, and then clean the data by applying a 5-fix median filter. We identify tidal cycles using data from Rijkswaterstaat, and split movement tracks into subsets lasting from one high tide to the next. Treating these subsets as individual replicates in a population, as recommended (???), we guess CTMM parameters and fit CTMM to each tidal cycle. Finally, we output the fitted models and their summaries as .Rdata and text files, respectively. Much of the preliminary processing also happens on the cluster, and uses the the WATLAS Utilities package. Workflow Prepare required libraries. Read in data, apply the cleaning function, and overwrite local data. 2.1 Prepare watlasUtils and other libraries # watlasUtils assumed installed from the previous step # if not, install from the github repo as shown below devtools::install_github(&quot;pratikunterwegs/watlasUtils&quot;, ref=&quot;devbranch&quot;) library(watlasUtils) # libraries to process data library(data.table) library(purrr) library(glue) library(fasttime) library(bit64) library(stringr) # libraries for the cluster library(ssh) 2.2 Prepare to remove attractor points, clean data and add tides # read in identified attractor points atp &lt;- fread(&quot;data/attractor_points.txt&quot;) # make a list of data files to read data_files &lt;- list.files(path = &quot;data/watlas/&quot;, pattern = &quot;whole_season*&quot;, full.names = TRUE) data_ids &lt;- str_extract(data_files, &quot;(tx_\\\\d+)&quot;) %&gt;% str_sub(-3,-1) # read deployment data from local file in data folder tag_info &lt;- fread(&quot;data/SelinDB.csv&quot;) # filter out NAs in release date and time tag_info &lt;- tag_info[!is.na(Release_Date) &amp; !is.na(Release_Time),] # make release date column as POSIXct tag_info[,Release_Date := as.POSIXct(paste(Release_Date, Release_Time, sep = &quot; &quot;), format = &quot;%d.%m.%y %H:%M&quot;, tz = &quot;CET&quot;)] # sub for knots in data data_files &lt;- data_files[as.integer(data_ids) %in% tag_info$Toa_Tag][1] # add tide data tides_2018 &lt;- &quot;data/tidesSummer2018.csv&quot; 2.3 Read, clean, and write data # read password password = fread(&quot;data/password.txt&quot;)$password # transfer files and process using ctmm for(i in 1:length(data_files)){ # connect to peregrine s &lt;- ssh_connect(&quot;p284074@peregrine.hpc.rug.nl&quot;, passwd = password) # make directory if non-existent ssh_exec_wait(s, command = &quot;mkdir -p data/watlas&quot;) # list files already present files_on_prg &lt;- ssh_exec_internal(s, command = &quot;ls data/watlas&quot;) files_on_prg &lt;- rawToChar(files_on_prg$stdout) %&gt;% str_split(&quot;\\n&quot;) %&gt;% unlist() # check name data_name &lt;- data_files[i] %&gt;% str_split(&quot;/&quot;) %&gt;% unlist() %&gt;% .[3] if(!data_name %in% files_on_prg){ # upload data file for processing scp_upload(s, data_files[i], to = &quot;data/watlas/&quot;) } # make job file { shebang &lt;- readLines(&quot;code/template_job.sh&quot;) # rename job shebang[2] &lt;- glue(&#39;#SBATCH --job-name=ctmm_{data_ids[i]}&#39;) text &lt;- glue(&#39;Rscript code/code_doCtmm.r {data_files[i]}&#39;) jobfile &lt;- glue(&#39;code/job_ctmm_{data_ids[i]}.sh&#39;) writeLines(c(shebang, text), con = jobfile) scp_upload(s, jobfile, to = &quot;code/&quot;) } ssh_exec_wait(s, command = glue(&#39;dos2unix {jobfile}&#39;)) # process using ctmm ssh_exec_wait(s, command = glue(&#39;sbatch {jobfile}&#39;)) # disconnect ssh_disconnect(s) } "],
["applying-ctmm-to-watlas-data.html", "Section 3 Applying CTMM to WATLAS data 3.1 Load libraries 3.2 Load prelim data 3.3 Choose scales of aggregation", " Section 3 Applying CTMM to WATLAS data Here, we examine the effect of different aggregation scales on CTMM fits and speed estimates. 3.1 Load libraries # load libs library(data.table) library(lubridate) library(sf) library(glue) library(stringr) library(fasttime) library(tibble) library(dplyr) library(purrr) library(tidyr) # devtools::install_github(&quot;pratikunterwegs/watlasUtils&quot;, ref = &quot;devbranch&quot;) library(watlasUtils) library(ctmm) # plot libs library(ggplot2) library(ggthemes) # ssh func library(ssh) 3.2 Load prelim data # load data # make a list of data files to read data_files &lt;- list.files(path = &quot;data/watlas/&quot;, pattern = &quot;whole_season*&quot;, full.names = TRUE) data_ids &lt;- str_extract(data_files, &quot;(tx_\\\\d+)&quot;) %&gt;% str_sub(-3,-1) # read deployment data from local file in data folder tag_info &lt;- fread(&quot;data/SelinDB.csv&quot;) # filter out NAs in release date and time tag_info &lt;- tag_info[!is.na(Release_Date) &amp; !is.na(Release_Time),] # make release date column as POSIXct tag_info[,Release_Date := as.POSIXct(paste(Release_Date, Release_Time, sep = &quot; &quot;), format = &quot;%d.%m.%y %H:%M&quot;, tz = &quot;CET&quot;)] # sub for knots in data data_files &lt;- data_files[as.integer(data_ids) %in% tag_info$Toa_Tag][1:10] data_ids &lt;- str_extract(data_files, &quot;(tx_\\\\d+)&quot;) %&gt;% str_sub(-3,-1) 3.3 Choose scales of aggregation scales &lt;- c(15, 30, 60) data_to_test &lt;- crossing(scales, nesting(data_files, data_ids)) rm(scales, data_files) # read password password = fread(&quot;data/password.txt&quot;)$password for(i in 1:nrow(data_to_test)){ scale &lt;- data_to_test$scales[i] file &lt;- data_to_test$data_files[i] id &lt;- data_to_test$data_ids[i] # connect to peregrine s &lt;- ssh_connect(&quot;p284074@peregrine.hpc.rug.nl&quot;, passwd = password) # make directory if non-existent ssh_exec_wait(s, command = &quot;mkdir -p data/watlas&quot;) # list files already present files_on_prg &lt;- ssh_exec_internal(s, command = &quot;ls data/watlas&quot;) files_on_prg &lt;- rawToChar(files_on_prg$stdout) %&gt;% str_split(&quot;\\n&quot;) %&gt;% unlist() # check name data_name &lt;- file %&gt;% str_split(&quot;/&quot;) %&gt;% unlist() %&gt;% .[3] if(!data_name %in% files_on_prg){ # upload data file for processing scp_upload(s, file, to = &quot;data/watlas/&quot;) } # make job file { shebang &lt;- readLines(&quot;code/template_job.sh&quot;) # rename job shebang[2] &lt;- glue(&#39;#SBATCH --job-name=ctmm_{file}&#39;) text &lt;- glue(&#39;Rscript code/code_test_ctmm_scale.r {file} {scale}&#39;) jobfile &lt;- glue(&#39;code/job_ctmm_{id}_{scale}.sh&#39;) writeLines(c(shebang, text), con = jobfile) scp_upload(s, jobfile, to = &quot;code/&quot;) } ssh_exec_wait(s, command = glue(&#39;dos2unix {jobfile}&#39;)) # process using ctmm ssh_exec_wait(s, command = glue(&#39;sbatch {jobfile}&#39;)) # disconnect ssh_disconnect(s) } # # ctmm section # { # # get the outliers but do not plot # outliers &lt;- map(tel, outlie, plot=FALSE) # # get a list of 99 th percentile outliers # q90 &lt;- map(outliers, function(this_outlier_set){ # quantile(this_outlier_set[[1]], probs = c(0.99)) # }) # # remove outliers from telemetry data # tel &lt;- pmap(list(tel, outliers, q90), # function(this_tel_obj, this_outlier_set, outlier_quantile) # {this_tel_obj[-(which(this_outlier_set[[1]] &gt;= outlier_quantile)),]}) # # some patches have no data remaining, filter them out # tel &lt;- keep(tel, function(this_tel){nrow(this_tel) &gt; 0}) # r_patches &lt;- length(tel) # } # ratio = r_patches/n_patches # message(ratio) # return(ratio) "]
]
